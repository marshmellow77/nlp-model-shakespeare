{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e763d4f7-9c0c-40ae-b225-f968b07b15b2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 98306, done.\u001b[K\n",
      "remote: Total 98306 (delta 0), reused 0 (delta 0), pack-reused 98306\u001b[K\n",
      "Receiving objects: 100% (98306/98306), 83.63 MiB | 40.57 MiB/s, done.\n",
      "Resolving deltas: 100% (71363/71363), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe6bd52b-5f18-41b2-9893-74e983345b9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/29/2022 14:18:25 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "01/29/2022 14:18:25 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=shakespeare-clm/runs/Jan29_14-18-25_default,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=shakespeare-clm,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=shakespeare-clm,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "01/29/2022 14:18:26 - WARNING - datasets.builder - Using custom data configuration default\n",
      "01/29/2022 14:18:26 - INFO - datasets.info - Loading Dataset Infos from /home/studio-lab-user/.cache/huggingface/modules/datasets_modules/datasets/tiny_shakespeare/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e\n",
      "01/29/2022 14:18:26 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "01/29/2022 14:18:26 - INFO - datasets.info - Loading Dataset info from /home/studio-lab-user/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e\n",
      "01/29/2022 14:18:26 - WARNING - datasets.builder - Reusing dataset tiny_shakespeare (/home/studio-lab-user/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "01/29/2022 14:18:26 - INFO - datasets.info - Loading Dataset info from /home/studio-lab-user/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 157.76it/s]\n",
      "[INFO|configuration_utils.py:644] 2022-01-29 14:18:26,346 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /home/studio-lab-user/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "[INFO|configuration_utils.py:680] 2022-01-29 14:18:26,346 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.17.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:336] 2022-01-29 14:18:26,478 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:644] 2022-01-29 14:18:26,609 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /home/studio-lab-user/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "[INFO|configuration_utils.py:680] 2022-01-29 14:18:26,609 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.17.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-01-29 14:18:27,584 >> loading file https://huggingface.co/distilgpt2/resolve/main/vocab.json from cache at /home/studio-lab-user/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-01-29 14:18:27,585 >> loading file https://huggingface.co/distilgpt2/resolve/main/merges.txt from cache at /home/studio-lab-user/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-01-29 14:18:27,585 >> loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer.json from cache at /home/studio-lab-user/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-01-29 14:18:27,585 >> loading file https://huggingface.co/distilgpt2/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-01-29 14:18:27,585 >> loading file https://huggingface.co/distilgpt2/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2022-01-29 14:18:27,585 >> loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:644] 2022-01-29 14:18:27,718 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /home/studio-lab-user/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
      "[INFO|configuration_utils.py:680] 2022-01-29 14:18:27,719 >> Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.17.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1427] 2022-01-29 14:18:27,952 >> loading weights file https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin from cache at /home/studio-lab-user/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
      "[INFO|modeling_utils.py:1694] 2022-01-29 14:18:30,666 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1702] 2022-01-29 14:18:30,666 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "01/29/2022 14:18:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-1827b1687189f2af.arrow\n",
      "01/29/2022 14:18:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-f70f3ce8349e3f9e.arrow\n",
      "01/29/2022 14:18:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-8d94a9acb3a1eecc.arrow\n",
      "01/29/2022 14:18:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-42750cfd73547177.arrow\n",
      "01/29/2022 14:18:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-bbda6109eda35cf2.arrow\n",
      "01/29/2022 14:18:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e/cache-eb763c30ddaf6452.arrow\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1252] 2022-01-29 14:18:37,342 >> ***** Running training *****\n",
      "[INFO|trainer.py:1253] 2022-01-29 14:18:37,342 >>   Num examples = 294\n",
      "[INFO|trainer.py:1254] 2022-01-29 14:18:37,342 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1255] 2022-01-29 14:18:37,342 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:1256] 2022-01-29 14:18:37,342 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "[INFO|trainer.py:1257] 2022-01-29 14:18:37,342 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1258] 2022-01-29 14:18:37,342 >>   Total optimization steps = 441\n",
      "100%|█████████████████████████████████████████| 441/441 [03:24<00:00,  1.98it/s][INFO|trainer.py:1481] 2022-01-29 14:22:02,336 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 205.0178, 'train_samples_per_second': 4.302, 'train_steps_per_second': 2.151, 'train_loss': 3.728642400970805, 'epoch': 3.0}\n",
      "100%|█████████████████████████████████████████| 441/441 [03:25<00:00,  2.15it/s]\n",
      "[INFO|trainer.py:2103] 2022-01-29 14:22:02,361 >> Saving model checkpoint to shakespeare-clm\n",
      "[INFO|configuration_utils.py:430] 2022-01-29 14:22:02,362 >> Configuration saved in shakespeare-clm/config.json\n",
      "[INFO|modeling_utils.py:1074] 2022-01-29 14:22:02,901 >> Model weights saved in shakespeare-clm/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2074] 2022-01-29 14:22:02,901 >> tokenizer config file saved in shakespeare-clm/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2080] 2022-01-29 14:22:02,901 >> Special tokens file saved in shakespeare-clm/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  train_loss               =     3.7286\n",
      "  train_runtime            = 0:03:25.01\n",
      "  train_samples            =        294\n",
      "  train_samples_per_second =      4.302\n",
      "  train_steps_per_second   =      2.151\n",
      "01/29/2022 14:22:02 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:2353] 2022-01-29 14:22:02,996 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2355] 2022-01-29 14:22:02,996 >>   Num examples = 17\n",
      "[INFO|trainer.py:2358] 2022-01-29 14:22:02,996 >>   Batch size = 2\n",
      "100%|█████████████████████████████████████████████| 9/9 [00:01<00:00,  6.29it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_loss               =     3.4572\n",
      "  eval_runtime            = 0:00:01.44\n",
      "  eval_samples            =         17\n",
      "  eval_samples_per_second =     11.804\n",
      "  eval_steps_per_second   =      6.249\n",
      "  perplexity              =    31.7294\n",
      "[INFO|modelcard.py:460] 2022-01-29 14:22:04,573 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'dataset': {'name': 'tiny_shakespeare', 'type': 'tiny_shakespeare', 'args': 'default'}}\n"
     ]
    }
   ],
   "source": [
    "!python transformers/examples/pytorch/language-modeling/run_clm.py \\\n",
    "    --model_name_or_path distilgpt2 \\\n",
    "    --dataset_name tiny_shakespeare \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --output_dir shakespeare-clm \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4620deab-3bfa-4c15-b649-8f0907f818cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc7ec22-6594-46cf-9595-37a54d8a634b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb680193353240cb80d7d1dbb4f43f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1e7b8e6d28456b936fa5ff1e24eb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/336M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66dba1479d154df8abe52a6dce9c1236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f42af25efb844848db446e5f343e5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907f5162087f47dbb67935d573af646e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "textgen_untrained = pipeline(\"text-generation\", model=\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a235bcc-8cec-4125-a4ee-be4acdc56012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is often not fully understood. While people may argue for the existence of a human being, they may argue for the existence of a non-human entity and thus the existence of a specific entity. As more research is needed and more research and more evidence becomes available, more research is needed to know more about evolution and how the universe is being created. For this reason we propose that we consider this method of searching for natural selection as the basis for the existence of a non-human\n"
     ]
    }
   ],
   "source": [
    "print(textgen_untrained(\"The meaning of life\", min_length=50, max_length=100)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3415c3ef-0453-4fc6-815b-3fcbd52bfb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "textgen_shakespeare = pipeline(\"text-generation\", \"./shakespeare-clm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0bb8650-a328-4268-8535-9421fe8da3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is now. The child makes love. Come, take thy hand, dear, to thy loving hand!\n",
      "\n",
      "HERO I, thou art sweetest friend, friend, dear mother,\n",
      "And if thou take thee, marry thy boy to his sweet head.\n",
      "\n",
      "GINGLEMEN:\n",
      "I think you can say your love you'll love.\n",
      "\n",
      "HERO:\n",
      "You may, too, be that sweetest friend to thy son.\n",
      "\n",
      "HERO\n"
     ]
    }
   ],
   "source": [
    "print(textgen_shakespeare(\"The meaning of life\", min_length=50, max_length=100)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ac03e-546d-48ad-a8ff-5dbbc6eb1976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
